{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CA02: Spam eMail Detection \n",
    "## using Naive BayesClassification Algorithm\n",
    "Arshak Mkhitaryan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "executionInfo": {
     "elapsed": 192,
     "status": "ok",
     "timestamp": 1644482172161,
     "user": {
      "displayName": "Arshak Mkhitaryan",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhuzTzwAyAuOk5OJhXFnrBoGzRpmwofejdGD_0g_A=s64",
      "userId": "13547081344309721700"
     },
     "user_tz": 480
    },
    "id": "IgOZ9N5rnYYH"
   },
   "outputs": [],
   "source": [
    "#import libraries\n",
    "import os\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#specifying the data paths\n",
    "train_data = './train-mails' \n",
    "test_data = './test-mails'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "'/' means root directory\n",
    "\n",
    "**'./' means current directory**\n",
    "\n",
    "'../' means parent directory\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a Dictionary of the most common words (function breakdown)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before writing the function let's use the training_data to understand the code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['./train-mails\\\\3-1msg1.txt',\n",
       " './train-mails\\\\3-1msg2.txt',\n",
       " './train-mails\\\\3-1msg3.txt',\n",
       " './train-mails\\\\3-375msg1.txt',\n",
       " './train-mails\\\\3-378msg1.txt']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_words = [] # creating an empty list for all of the words from the emails\n",
    "emails = [os.path.join(train_data,f) for f in os.listdir(train_data)] # list comprehension, creating a list of \n",
    "emails[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['./train-mails\\\\3-1msg1.txt',\n",
       " './train-mails\\\\3-1msg2.txt',\n",
       " './train-mails\\\\3-1msg3.txt',\n",
       " './train-mails\\\\3-375msg1.txt',\n",
       " './train-mails\\\\3-378msg1.txt']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# the same can be written as\n",
    "emails = []\n",
    "for f in os.listdir(train_data):\n",
    "    file_path = os.path.join(train_data,f)\n",
    "    emails.append(file_path)\n",
    "emails[0:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice the delimiter '\\\\\\\\' in the path './train-mails\\\\\\\\3-1msg1.txt'. If your delimeter is different you might have to change it in the function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subject: re : 6 . 199 ipa\n",
      "\n",
      "why must kind stuff decide vote ? since obviously ipa membership equal interest matter never , achieve us join ipa force issue , \" pack \" , never representative . why let invisible hand \" market \" idea operate freely instead ? fewer fewer ipa 's made-up symbol , either organization become completely irrelevant , own mind respond \" market force \" , perhap group step propose system manifestly better anyone else 's , achieve standardization . really few top name phonetics together editor few journal , probably something . simply anywhere else : someone publish truly superior system start . finally , reality , seem certain trend occur anyway particular resistance hachek correspond ipa symbol wane . put inconsistency american v . canadian v . british spell , probably here . gonna worry something , worry those case where te same symbol different commonly meaning ' j ' ' y ' . alexis mr\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# looking at one file\n",
    "f = open('./train-mails\\\\6-380msg1.txt', \"r\")\n",
    "print(f.read())\n",
    "# we can see that the fist line is the subject"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Subject:', 're', ':', '2', '.', '882', 's', '-', '>', 'np', 'np', '>', 'deat', ':', 'sun', ',', '15', 'dec', '91', '2']\n"
     ]
    }
   ],
   "source": [
    "#getting all the words from all emails\n",
    "all_words = []\n",
    "for mail in emails: #loop through all the emails\n",
    "    with open(mail) as m: #open each email\n",
    "        for line in m: #loop through each line\n",
    "            words = line.split() #split lines into words separated by a whitespace\n",
    "            all_words += words #append each word to the list of all words\n",
    "print(all_words[0:20])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the subject line is included, let's fix that"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['>', 'deat', ':', 'sun', ',', '15', 'dec', '91', '2', ':', '25', ':', '2', 'est', '>', ':', 'michael', '<', 'mmorse', '@']\n"
     ]
    }
   ],
   "source": [
    "all_words = []\n",
    "for mail in emails: #loop through all the emails\n",
    "    with open(mail) as m: #open each email\n",
    "        for i, line in enumerate(m): #loop through each line\n",
    "            if i == 2: #use only the 3rd line [2]################################################# this is the fix\n",
    "                words = line.split() #split lines into words separated by a whitespace\n",
    "                all_words += words #append each word to the list of all words\n",
    "print(all_words[0:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'>': 1605,\n",
       " 'deat': 50,\n",
       " ':': 5011,\n",
       " 'sun': 17,\n",
       " ',': 18338,\n",
       " '15': 208,\n",
       " 'dec': 18,\n",
       " '91': 4,\n",
       " '2': 866,\n",
       " '25': 134}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# creating a counter object\n",
    "dictionary = Counter(all_words) #Counter() creates a dictionary {'element': count}, e.g., {'success1': 169, 'plan': 169}\n",
    "dict(list(dictionary.items())[0:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['>', 'deat', ':', 'sun', ',', '15', 'dec', '91', '2', '25']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# but we can't iterate through dictionaries, that's why we put those values in a list\n",
    "list_to_remove = list(dictionary) #creating a list of keys (or words only) without the counts\n",
    "list_to_remove[0:10]\n",
    "#notice how each value is uniaue, because the list is based on the Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'deat': 50,\n",
       " 'sun': 17,\n",
       " 'dec': 18,\n",
       " 'est': 21,\n",
       " 'michael': 36,\n",
       " 'mmorse': 1,\n",
       " 'yorku': 1,\n",
       " 'ca': 126,\n",
       " 'subject': 209,\n",
       " 're': 190}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# let's use our list_to_remove list to remove numbers and single characters\n",
    "for item in list_to_remove: # loop through each word\n",
    "    if item.isalpha() == False: # if contains something other than letters\n",
    "      del dictionary[item] # delete an item from the Counter object with the key from our list\n",
    "    elif len(item) == 1: # single letters, characters?\n",
    "      del dictionary[item]\n",
    "\n",
    "dict(list(dictionary.items())[0:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the last thing the function does is it picks only 3000 of the most common words\n",
    "dictionary = dictionary.most_common(3000) # .most_common is a method of Counter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### All above steps in a function make_Dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "executionInfo": {
     "elapsed": 180,
     "status": "ok",
     "timestamp": 1644482231944,
     "user": {
      "displayName": "Arshak Mkhitaryan",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhuzTzwAyAuOk5OJhXFnrBoGzRpmwofejdGD_0g_A=s64",
      "userId": "13547081344309721700"
     },
     "user_tz": 480
    },
    "id": "VdhbByKXxune"
   },
   "outputs": [],
   "source": [
    "#make_Dictionary (modified)\n",
    "# I added an enumerator and an if statement to only select the message of the email\n",
    "def make_Dictionary(root_dir):\n",
    "  all_words = [] #create an empty list for all words\n",
    "  emails = [os.path.join(root_dir,f) for f in os.listdir(root_dir)] #list comprehension for all of the file paths\n",
    "  for mail in emails: #loop through all the emails\n",
    "    with open(mail) as m: #open each email\n",
    "      for i, line in enumerate(m): #loop through each line\n",
    "        if i == 2: # use only the 3rd line [2]\n",
    "            words = line.split() #split lines into words separated by a whitespace\n",
    "            all_words += words #append each word to the list of all words\n",
    "            \n",
    "  dictionary = Counter(all_words) #Counter creates a dictionary {'element': count}, e.g., {'success1': 169, 'plan': 169}\n",
    "\n",
    "  list_to_remove = list(dictionary) #creating a list of key (or words only) without the counts\n",
    "    \n",
    "  for item in list_to_remove: # loop through each word\n",
    "    if item.isalpha() == False: # if contains something other than letters\n",
    "      del dictionary[item] # delete an item from the Counter object with the key from our list\n",
    "    elif len(item) == 1: # single letters, characters?\n",
    "      del dictionary[item]\n",
    "  dictionary = dictionary.most_common(3000) # .most_common is a method of Counter\n",
    "  return dictionary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating feature columns and labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "executionInfo": {
     "elapsed": 165,
     "status": "ok",
     "timestamp": 1644482454814,
     "user": {
      "displayName": "Arshak Mkhitaryan",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhuzTzwAyAuOk5OJhXFnrBoGzRpmwofejdGD_0g_A=s64",
      "userId": "13547081344309721700"
     },
     "user_tz": 480
    },
    "id": "fcCKCQziyAsi"
   },
   "outputs": [],
   "source": [
    "# extract_features() modified\n",
    "# deleted unnecesary lines\n",
    "# substituted startswith and split('/') by 'if 'spmsg' in fil:' as a more robust solution on different platforms\n",
    "# since my os had a diffrent delimiter '\\\\'\n",
    "# deleted 'count = count + 1' since it was never used in the code\n",
    "def extract_features(mail_dir):\n",
    "  files = [os.path.join(mail_dir,fi) for fi in os.listdir(mail_dir)] # getting the file paths\n",
    "  features_matrix = np.zeros((len(files),3000)) # creating an empty features matrix\n",
    "  train_labels = np.zeros(len(files)) #creating an empty series for labels\n",
    "  docID = 0\n",
    "    \n",
    "  for fil in files: # loop through each file\n",
    "    with open(fil) as fi:   # open each file\n",
    "      for i, line in enumerate(fi): # loop through each line, use only line 3 [2]   \n",
    "        if i ==2:\n",
    "          words = line.split() # split on whitespace \n",
    "          for word in words: # lop through each word\n",
    "            wordID = 0 # setting an initial value\n",
    "            for i, d in enumerate(dictionary):\n",
    "              if d[0] == word: # d[0] selects the actual word (key) from the dictionary and compares it with the word\n",
    "                wordID = i # wordID is the same as enummeration number in the dictionary\n",
    "                features_matrix[docID,wordID] = words.count(word) # docID as a row, wordID as a column, we count how many times a word appeared in a document\n",
    "      train_labels[docID] = 0; # non spam = 0 by default\n",
    "    \n",
    "      if 'spmsg' in fil: # if 'spmsg' in the filename (filepath)\n",
    "        train_labels[docID] = 1 # then change the label to 1\n",
    "      docID = docID + 1 # increase document id for the next loop, but we could use enumerate\n",
    "  return features_matrix, train_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Running preprocessing functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### make_Dictioinary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating a training dictionary...Done!\n"
     ]
    }
   ],
   "source": [
    "# Running preprocessing functions\n",
    "print('Creating a training dictionary...', end='')\n",
    "dictionary = make_Dictionary(train_data)\n",
    "print('Done!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### extract_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reading and processing emails from TRAIN and TEST folders\n",
      "Creating a feature_matrix for training data...Done!\n",
      "Creating a feature_matrix for testing data...Done!\n"
     ]
    }
   ],
   "source": [
    "print (\"reading and processing emails from TRAIN and TEST folders\")\n",
    "print('Creating a feature_matrix for training data...', end='')\n",
    "features_matrix, labels = extract_features(train_data) # extracts feature matrix and labels and saves into the respective variables\n",
    "print('Done!')\n",
    "print('Creating a feature_matrix for testing data...', end='')\n",
    "test_features_matrix, test_labels = extract_features(test_data)\n",
    "print('Done!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "       [ 1.,  1.,  0., ...,  0.,  0.,  0.],\n",
       "       [ 1.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "       ...,\n",
       "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "       [17.,  2.,  0., ...,  0.,  0.,  0.]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#let's take a look at the test_features_matrix\n",
    "test_features_matrix # the numbers represent the counts of words (columns) in documents (rows)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "qBP1MsrKzjd6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Model using Gaussian Naibe Bayes algorithm .....\n",
      "Training completed\n"
     ]
    }
   ],
   "source": [
    "model = GaussianNB() # we choose Gaussian model, because we assume normal distribution\n",
    "print (\"Training Model using Gaussian Naibe Bayes algorithm .....\")\n",
    "model.fit(features_matrix, labels) # we fit the model X = feature_matrix, y = labels \n",
    "print (\"Training completed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calassifying"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "qBP1MsrKzjd6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "testing trained model to predict Test Data labels\n",
      "Completed classification of the Test Data\n"
     ]
    }
   ],
   "source": [
    "print (\"testing trained model to predict Test Data labels\")\n",
    "predicted_labels = model.predict(test_features_matrix) # predicting labels using .predict model\n",
    "print (\"Completed classification of the Test Data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9615384615384616\n"
     ]
    }
   ],
   "source": [
    "print (accuracy_score(test_labels, predicted_labels)) # calculates the % accuracy comparing predicted labels with test labels"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyNY6EPjgxMftzzjIu0KX6n4",
   "collapsed_sections": [],
   "name": "CA02.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
